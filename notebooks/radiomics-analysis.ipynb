{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import SimpleITK as sitk\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DeJavu Serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"Times New Roman\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = pathlib.Path(\"..\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_and_features(rater: str = \"900\", label: int = 1, from_image: str = \"t2w\") -> tuple:\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the given label and returns the labels and features as separate dataframes.\n",
    "\n",
    "    :param rater: The rater identifier. Default is \"900\".\n",
    "    :type rater: str\n",
    "    :param label: A number from 1 to 5 indicating the disc of interest. Default is 1.\n",
    "    :type label: bool\n",
    "    :return: A tuple containing the labels and features.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    labels_df = pd.read_csv(root_dir.joinpath(\"data\", f\"midasdisclabels{rater}.csv\"), sep=\",\")\n",
    "    labels_df.dropna(inplace=True)\n",
    "    labels_df.rename(columns={\"subject_ID\": \"Subject_XNAT\", \"ID\": \"Session_XNAT\"}, inplace=True)\n",
    "\n",
    "    midas_img_relation = pd.read_csv(root_dir.joinpath(\"data\", \"filtered_midas900_t2w.csv\"), sep=\",\")\n",
    "    midas_img_relation[\"Subject_MIDS\"] = midas_img_relation[\"Image\"].map(lambda x: x.split(\"/\")[8])\n",
    "    midas_img_relation[\"Session_MIDS\"] = midas_img_relation[\"Image\"].map(lambda x: x.split(\"/\")[9])\n",
    "    midas_img_relation[\"Subject_XNAT\"] = midas_img_relation[\"Subject_MIDS\"].map(lambda x: f\"ceibcs_S{int(x.split('sub-S')[1])}\")\n",
    "    midas_img_relation[\"Session_XNAT\"] = midas_img_relation[\"Session_MIDS\"].map(lambda x: f\"ceibcs_E{int(x.split('ses-E')[1])}\")\n",
    "\n",
    "    id_labels = labels_df.merge(midas_img_relation, on=[\"Subject_XNAT\", \"Session_XNAT\"])\n",
    "    id_labels.rename(\n",
    "        columns={\n",
    "            \"L5-S\": \"1\",\n",
    "            \"L4-L5\": \"2\",\n",
    "            \"L3-L4\": \"3\",\n",
    "            \"L2-L3\": \"4\",\n",
    "            \"L1-L2\": \"5\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    radiomic_features = pd.read_csv(root_dir.joinpath(\"data\", f\"filtered_midas900_{from_image}_radiomics.csv\"), sep=\",\")\n",
    "    radiomic_features.rename(columns={\"Unnamed: 0\": \"ID\"}, inplace=True)\n",
    "\n",
    "    data = id_labels.merge(radiomic_features, on=\"ID\")\n",
    "\n",
    "    data = data.rename(columns={str(label): f\"label{label}\", \"ID\": f\"label{label}ID\"})\n",
    "    columns_mask = data.columns.str.contains(f\"label{label}\") & ~data.columns.str.contains(\"Configuration\")\n",
    "    data = data.loc[:, columns_mask]\n",
    "    data = data.rename(columns={f\"label{label}\": \"label\", f\"label{label}ID\": \"ID\"})\n",
    "\n",
    "    label_data = data.dropna(axis=0, how=\"any\")\n",
    "    label_data = label_data.loc[label_data[\"label\"] != 0]\n",
    "    label_data[\"ID\"] = label_data[\"ID\"].map(lambda x: x + str(label))\n",
    "    label_data = label_data.set_index(\"ID\")\n",
    "    labels = label_data[\"label\"]\n",
    "    features = label_data[label_data.select_dtypes(include=\"number\").columns.tolist()].drop(columns=\"label\")\n",
    "    return labels, features\n",
    "\n",
    "\n",
    "def get_labels_and_features_all_discs(rater: str = \"900\", verbose: bool = False, from_image: str = \"t1w_t2w\") -> tuple:\n",
    "    \"\"\"\n",
    "    Get labels and features for all discs.\n",
    "\n",
    "    :param rater: The rater identifier. Default is \"900\".\n",
    "    :type rater: str\n",
    "    :param verbose: Whether to print additional information and plot the label distribution. Default is False.\n",
    "    :type verbose: bool\n",
    "    :return: A tuple containing the labels and features.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    for label in range(1, 6):\n",
    "        labels_i, features_i = get_labels_and_features(rater=rater, label=label, from_image=from_image)\n",
    "        labels.append(labels_i)\n",
    "        features_i = features_i.rename(columns={name: name.replace(f\"label{label}_\", \"\") for name in features_i.columns.to_list()})\n",
    "        features.append(features_i)\n",
    "    features = pd.concat(features, axis=0)\n",
    "    labels = pd.concat(labels, axis=0)\n",
    "    if verbose:\n",
    "        print(f\"Labels shape: {labels.shape}, Features shape: {features.shape}\")\n",
    "        labels.plot(kind=\"hist\", xticks=[1, 2, 3, 4, 5], title=\"Label distribution\")\n",
    "        plt.show()\n",
    "    return labels, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "class VarianceFeatureReduction(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    VarianceFeatureReduction is a transformer that reduces the feature space by removing features with low variance.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    threshold : float, optional (default=0.05)\n",
    "        The threshold below which features will be removed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threshold=0.05):\n",
    "        self.threshold = threshold\n",
    "        self.selector = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the VarianceFeatureReduction transformer to the input data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like, shape (n_samples,), optional (default=None)\n",
    "            The target values.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        self.selector = VarianceThreshold(threshold=self.threshold)\n",
    "        self.selector.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Transform the input data by removing features with low variance.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like, shape (n_samples,), optional (default=None)\n",
    "            The target values.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        X_ : array-like, shape (n_samples, n_selected_features)\n",
    "            The transformed data with low variance features removed.\n",
    "        \"\"\"\n",
    "        X_ = X.copy()\n",
    "        X_ = X_.loc[:, self.selector.get_support()]\n",
    "        return X_\n",
    "\n",
    "\n",
    "class CorrelationFeatureReduction(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer class for reducing features based on correlation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    threshold : float, optional (default=0.8)\n",
    "        The threshold above which features will be removed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threshold=0.8):\n",
    "        self.threshold = threshold\n",
    "        self.corr_matrix_var = None\n",
    "        self.to_keep = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer to the input data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas DataFrame\n",
    "            The input data.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : CorrelationFeatureReduction\n",
    "            The fitted transformer object.\n",
    "\n",
    "        \"\"\"\n",
    "        self.corr_matrix_var = X.corr(method=\"spearman\").abs()  # absolute correlation matrix\n",
    "\n",
    "        # Initialize the flag vector with True values\n",
    "        self.to_keep = np.full((self.corr_matrix_var.shape[1]), True, dtype=bool)\n",
    "\n",
    "        for i in range(self.corr_matrix_var.shape[1]):\n",
    "            for j in range(i + 1, self.corr_matrix_var.shape[1]):\n",
    "                if self.to_keep[i] and self.corr_matrix_var.iloc[i, j] >= self.threshold:\n",
    "                    if self.to_keep[j]:\n",
    "                        self.to_keep[j] = False\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Transform the input data by removing highly correlated features.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas DataFrame\n",
    "            The input data.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        X_ : pandas DataFrame\n",
    "            The transformed data with highly correlated features removed.\n",
    "\n",
    "        \"\"\"\n",
    "        X_ = X.copy()\n",
    "        X_ = X_.iloc[:, self.to_keep]\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def test_multiple_models(features, labels):\n",
    "    # Define classifiers to test\n",
    "    classifiers = {\n",
    "        \"Random Forest\": RandomForestClassifier(),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "        \"SVM\": SVC(),\n",
    "        \"Logistic Regression\": LogisticRegression(),\n",
    "        \"Stochastic Gradient Descent\": SGDClassifier(),\n",
    "        \"Naive Bayes\": GaussianNB(),\n",
    "        \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "        \"Multilayer Perceptron\": MLPClassifier(),\n",
    "        \"AdaBoost\": AdaBoostClassifier(),\n",
    "        \"ExtraTrees\": ExtraTreesClassifier(),\n",
    "    }\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.25, random_state=0, stratify=labels)\n",
    "\n",
    "    # Test each classifier\n",
    "    f1_scores = {}\n",
    "    for name, clf in classifiers.items():\n",
    "        pipeline = Pipeline(\n",
    "            [\n",
    "                (\"variancethreshold\", VarianceFeatureReduction(threshold=0.05)),\n",
    "                (\"correlationreduction\", CorrelationFeatureReduction()),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"classifier\", clf),\n",
    "            ]\n",
    "        )\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        f1_scores[name] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    # Select the classifier with the highest F1 score\n",
    "    best_classifier = max(f1_scores, key=f1_scores.get)  # type: ignore\n",
    "    print(\"Best classifier:\", best_classifier)\n",
    "    print(\"F1 score:\", f1_scores[best_classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "\n",
    "def cv(clf, features, labels):\n",
    "    # Create a stratified 5-fold cross-validation object\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    pipeline_clf = Pipeline(\n",
    "        [\n",
    "            (\"variancethreshold\", VarianceFeatureReduction(threshold=0.05)),\n",
    "            (\"correlationreduction\", CorrelationFeatureReduction()),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"classifier\", clf),\n",
    "        ]\n",
    "    )\n",
    "    scores = cross_val_score(pipeline_clf, features, labels, cv=skf, scoring=\"f1_weighted\")\n",
    "    print(f\"Cross Validation F1 Score: {scores.mean():0.4f} +/- {scores.std():0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "def imbalanced_learning_suite(features, labels):\n",
    "    # Define classifiers to test\n",
    "    classifiers = {\n",
    "        \"Balanced Bagging Classifier\": BalancedBaggingClassifier(sampler=RandomUnderSampler()),\n",
    "        \"Balanced RandomForest Classifier\": BalancedRandomForestClassifier(),\n",
    "        \"RUS Boost Classifier\": RUSBoostClassifier(),\n",
    "        \"Easy Ensemble Classifier\": EasyEnsembleClassifier(),\n",
    "    }\n",
    "\n",
    "    # Create a stratified 5-fold cross-validation object\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for name, clf in classifiers.items():\n",
    "        pipeline_clf = Pipeline(\n",
    "            [\n",
    "                (\"variancethreshold\", VarianceFeatureReduction(threshold=0.05)),\n",
    "                (\"correlationreduction\", CorrelationFeatureReduction()),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"classifier\", clf),\n",
    "            ]\n",
    "        )\n",
    "        scores = cross_val_score(pipeline_clf, features, labels, cv=skf, scoring=\"f1_weighted\")\n",
    "        print(f\"{name}: {scores.mean():0.2f} f1 with a standard deviation of {scores.std():0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yellowbrick.classifier as viz\n",
    "\n",
    "from yellowbrick.style import set_palette\n",
    "\n",
    "set_palette(\"flatui\")\n",
    "\n",
    "\n",
    "def visual_metrics(clf, features, labels, classes=[\"1\", \"2\", \"3\", \"4\", \"5\"]):\n",
    "    labels_ = labels.copy()\n",
    "    if min(labels_) != 0:\n",
    "        labels_ = labels_ - min(labels_)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels_, test_size=0.25, random_state=0, stratify=labels_)\n",
    "\n",
    "    _, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    labels.plot(kind=\"hist\", title=\"Pfirmann grade distribution\", ax=axes[0], xticks=[1, 2, 3, 4, 5], align=\"mid\")\n",
    "\n",
    "    pipeline_clf = Pipeline(\n",
    "        [\n",
    "            (\"variancethreshold\", VarianceFeatureReduction(threshold=0.05)),\n",
    "            (\"correlationreduction\", CorrelationFeatureReduction()),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"classifier\", clf),\n",
    "        ]\n",
    "    )\n",
    "    pipeline_clf.fit(X_train, y_train)\n",
    "    axes[1].set_title(\"Classification Report\")\n",
    "    axes[1].set_ylabel(\"Class\")\n",
    "    visualizer_class = viz.ClassificationReport(pipeline_clf, classes=classes[::-1], support=True, ax=axes[1], cmap=\"Blues\")\n",
    "    visualizer_class.score(X_test, y_test)\n",
    "\n",
    "    axes[2].set_title(\"Classification Prediction Error\")\n",
    "    axes[2].set_xlabel(\"Class\")\n",
    "    axes[2].set_ylabel(\"Number of Predictions\")\n",
    "    visualizer_pred = viz.ClassPredictionError(pipeline_clf, classes=classes, ax=axes[2])\n",
    "    visualizer_pred.score(X_test, y_test)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "def random_search(clf, distribution, features, labels):\n",
    "    pipeline_clf = Pipeline(\n",
    "        [\n",
    "            (\"variancethreshold\", VarianceFeatureReduction(threshold=0.05)),\n",
    "            (\"correlationreduction\", CorrelationFeatureReduction()),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"classifier\", clf),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    rs_clf = RandomizedSearchCV(pipeline_clf, distribution, cv=skf, scoring=\"f1_weighted\", n_iter=10, random_state=0)\n",
    "    search = rs_clf.fit(features, labels)\n",
    "\n",
    "    print(f\"Best parameter (CV score={search.best_score_:0.3f}): {search.best_params_}\")\n",
    "    return {key.replace(\"classifier__\", \"\"): value for key, value in search.best_params_.items() if key.startswith(\"classifier__\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Near-zero variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Feature reduction based on variance thresholding (remove features with variance smaller than 0.05)\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Initialize selector based on VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.05)\n",
    "\n",
    "#  Estimate variances and reduce features\n",
    "labels, radiomic_features = get_labels_and_features_all_discs(rater=\"JDCarlos\")\n",
    "selector.fit_transform(radiomic_features)\n",
    "\n",
    "# Get the selected feature labels and reduce the Radiomic_Feature dataframe\n",
    "radiomic_features_var = radiomic_features.loc[:, selector.get_support()]\n",
    "\n",
    "# Display the number of features removed\n",
    "print(f\"{np.count_nonzero(~selector.get_support())}/{radiomic_features.shape[1]} features were removed due to near-zero variance.\")\n",
    "\n",
    "del selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_var = radiomic_features_var.corr(method=\"spearman\").abs()  # absolute correlation matrix\n",
    "\n",
    "# Initialize the flag vector with True values\n",
    "to_keep = np.full((corr_matrix_var.shape[1]), True, dtype=bool)\n",
    "\n",
    "for i in range(corr_matrix_var.shape[1]):\n",
    "    for j in range(i + 1, corr_matrix_var.shape[1]):\n",
    "        if to_keep[i] and corr_matrix_var.iloc[i, j] >= 0.8:\n",
    "            if to_keep[j]:\n",
    "                to_keep[j] = False\n",
    "\n",
    "# Retain features that are not higly correlated\n",
    "radiomic_features_corr = radiomic_features_var.iloc[:, to_keep]\n",
    "\n",
    "print(\n",
    "    f\"{np.count_nonzero(~to_keep)}/{radiomic_features_var.shape[1]} features were removed due to high correlation. {radiomic_features_corr.shape[1]} features remaining.\"\n",
    ")\n",
    "\n",
    "del to_keep, i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Calculate the correlation matrix of the original feature set\n",
    "corr_matrix = radiomic_features.corr(method=\"spearman\")\n",
    "# Display the correlation matrix\n",
    "plt.figure(figsize=(8, 6.5))\n",
    "sns.heatmap(corr_matrix, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation of Radiomic features\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate the correlation matrix of the reduced feature set\n",
    "corr_matrix_red = radiomic_features_corr.corr(method=\"spearman\")\n",
    "# Display the correlation matrix\n",
    "plt.figure(figsize=(8, 6.5))\n",
    "sns.heatmap(corr_matrix_red, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation of reduced radiomic features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from scipy.stats import zscore\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "radiomic_features_clus = copy.deepcopy(radiomic_features_corr)\n",
    "# Normalize the data\n",
    "radiomic_features_clus = zscore(radiomic_features_clus, axis=0)\n",
    "\n",
    "# Calculate and plot the clustergram\n",
    "row_linkage = hierarchy.linkage(distance.pdist(radiomic_features_clus.to_numpy()), method=\"ward\")\n",
    "col_linkage = hierarchy.linkage(distance.pdist(radiomic_features_clus.T.to_numpy()), method=\"ward\")\n",
    "g = sns.clustermap(\n",
    "    radiomic_features_clus, row_linkage=row_linkage, col_linkage=col_linkage, method=\"ward\", vmin=-3, vmax=3, figsize=(8, 10), cmap=\"viridis\"\n",
    ")\n",
    "g.ax_cbar.set_position((0.90, 0.2, 0.03, 0.3))\n",
    "\n",
    "# Extract 5 disc degeneration clusters and append the \"Clusters\" variable to the DataFrame\n",
    "n_clusters = 5\n",
    "radiomic_features_clus[\"Clusters\"] = fcluster(row_linkage, n_clusters, criterion=\"maxclust\")\n",
    "\n",
    "# Print the cluster assignments\n",
    "print(\"Cluster Assignments:\", radiomic_features_clus[\"Clusters\"])\n",
    "\n",
    "del g, n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Concatenate the target clinical variable to the radiomic DataFrame\n",
    "radiomic_features_clus = pd.concat([radiomic_features_clus, labels], axis=1)\n",
    "\n",
    "# Barplot clusters/grades\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.countplot(x=\"Clusters\", hue=\"label\", data=radiomic_features_clus, palette=\"coolwarm\")\n",
    "plt.title(\"Distribution of Pfirmann grade in each Cluster\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Pfirmann grade\", loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "# Perform chi-squared test\n",
    "chi2, p, _, _ = chi2_contingency(pd.crosstab(radiomic_features_clus[\"label\"], radiomic_features_clus[\"Clusters\"]))\n",
    "\n",
    "# Print the results\n",
    "print(f\"Chi-squared statistic: {chi2}\")\n",
    "print(f\"P-value: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_results(clf, disc: int = 1):\n",
    "    labels, features = get_labels_and_features(rater=\"JDCarlos\", label=disc)\n",
    "    cv(clf, features, labels)\n",
    "    visual_metrics(clf, features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in range(1, 6):\n",
    "    print(f\"Disc: {label}\")\n",
    "    labels, features = get_labels_and_features(rater=\"JDCarlos\", label=label)\n",
    "    test_multiple_models(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Disc 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "disc_results(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disc 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier()\n",
    "disc_results(clf, disc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disc 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier()\n",
    "disc_results(clf, disc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disc 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "disc_results(clf, disc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disc 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "disc_results(clf, disc=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All discs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_discs_results(clf):\n",
    "    labels, features = get_labels_and_features_all_discs(rater=\"JDCarlos\")\n",
    "    cv(clf, features, labels)\n",
    "    visual_metrics(clf, features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, features = get_labels_and_features_all_discs(rater=\"JDCarlos\")\n",
    "test_multiple_models(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = {\n",
    "    \"classifier__loss\": [\"log_loss\", \"exponential\"],\n",
    "    \"classifier__learning_rate\": [0.01, 0.1, 0.3, 0.5],\n",
    "    \"classifier__n_estimators\": [10, 50, 100],\n",
    "    \"classifier__max_depth\": [1, 5, 10],\n",
    "    \"classifier__max_features\": [None, \"sqrt\", \"log2\"],\n",
    "}\n",
    "labels, features = get_labels_and_features_all_discs(rater=\"JDCarlos\")\n",
    "best_params = random_search(GradientBoostingClassifier(), distribution, features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "all_discs_results(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, features = get_labels_and_features_all_discs(rater=\"JDCarlos\")\n",
    "imbalanced_learning_suite(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining classes 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_results_combining_1_and_2(clf, disc: int = 1):\n",
    "    labels, features = get_labels_and_features(rater=\"JDCarlos\", label=disc)\n",
    "    labels.loc[labels == 1] = 2\n",
    "    cv(clf, features, labels)\n",
    "    visual_metrics(clf, features, labels, classes=[\"1 and 2\", \"3\", \"4\", \"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in range(1, 6):\n",
    "    print(f\"Disc: {label}\")\n",
    "    labels, features = get_labels_and_features(rater=\"JDCarlos\", label=label)\n",
    "    labels.loc[labels == 1] = 2\n",
    "    test_multiple_models(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disc 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier()\n",
    "disc_results_combining_1_and_2(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disc 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier()\n",
    "disc_results_combining_1_and_2(clf, disc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disc 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier()\n",
    "disc_results_combining_1_and_2(clf, disc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disc 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "disc_results_combining_1_and_2(clf, disc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disc 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "disc_results_combining_1_and_2(clf, disc=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All discs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_discs_results_combining_1_and_2(clf):\n",
    "    labels, features = get_labels_and_features_all_discs(rater=\"JDCarlos\")\n",
    "    labels.loc[labels == 1] = 2\n",
    "    cv(clf, features, labels)\n",
    "    visual_metrics(clf, features, labels, classes=[\"1 and 2\", \"3\", \"4\", \"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, features = get_labels_and_features_all_discs(rater=\"JDCarlos\")\n",
    "labels.loc[labels == 1] = 2\n",
    "test_multiple_models(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = {\n",
    "    \"classifier__loss\": [\"log_loss\", \"exponential\"],\n",
    "    \"classifier__learning_rate\": [0.01, 0.1, 0.3, 0.5],\n",
    "    \"classifier__n_estimators\": [10, 50, 100],\n",
    "    \"classifier__max_depth\": [1, 5, 10],\n",
    "    \"classifier__max_features\": [None, \"sqrt\", \"log2\"],\n",
    "}\n",
    "labels, features = get_labels_and_features_all_discs(rater=\"JDCarlos\")\n",
    "labels.loc[labels == 1] = 2\n",
    "best_params = random_search(RandomForestClassifier(), distribution, features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "all_discs_results_combining_1_and_2(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, features = get_labels_and_features_all_discs(rater=\"JDCarlos\")\n",
    "labels.loc[labels == 1] = 2\n",
    "imbalanced_learning_suite(features, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radiomics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
