{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import SimpleITK as sitk\n",
    "from tqdm import tqdm\n",
    "\n",
    "from radiomics import imageoperations\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DeJavu Serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"Times New Roman\"]\n",
    "\n",
    "colors = [\"#663171\", \"#ea7428\", \"#0c7156\", \"#cf3a36\", \"#e2998a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = pathlib.Path(\"..\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe_from_csv(\n",
    "    rater: str = \"900\", from_image: str = \"t2w\"\n",
    ") -> pd.DataFrame:\n",
    "    labels_df = pd.read_csv(\n",
    "        root_dir.joinpath(\"data\", f\"midasdisclabels{rater}.csv\"), sep=\",\"\n",
    "    )\n",
    "    labels_df.dropna(inplace=True)\n",
    "    labels_df.rename(\n",
    "        columns={\"subject_ID\": \"Subject_XNAT\", \"ID\": \"Session_XNAT\"}, inplace=True\n",
    "    )\n",
    "\n",
    "    midas_img_relation = pd.read_csv(\n",
    "        root_dir.joinpath(\"data\", \"filtered_midas900_t2w.csv\"), sep=\",\"\n",
    "    )\n",
    "    midas_img_relation[\"Subject_MIDS\"] = midas_img_relation[\"Image\"].map(\n",
    "        lambda x: x.split(\"/\")[8]\n",
    "    )\n",
    "    midas_img_relation[\"Session_MIDS\"] = midas_img_relation[\"Image\"].map(\n",
    "        lambda x: x.split(\"/\")[9]\n",
    "    )\n",
    "    midas_img_relation[\"Subject_XNAT\"] = midas_img_relation[\"Subject_MIDS\"].map(\n",
    "        lambda x: f\"ceibcs_S{int(x.split('sub-S')[1])}\"\n",
    "    )\n",
    "    midas_img_relation[\"Session_XNAT\"] = midas_img_relation[\"Session_MIDS\"].map(\n",
    "        lambda x: f\"ceibcs_E{int(x.split('ses-E')[1])}\"\n",
    "    )\n",
    "\n",
    "    id_labels = labels_df.merge(midas_img_relation, on=[\"Subject_XNAT\", \"Session_XNAT\"])\n",
    "    id_labels.rename(\n",
    "        columns={\n",
    "            \"L5-S\": \"1\",\n",
    "            \"L4-L5\": \"2\",\n",
    "            \"L3-L4\": \"3\",\n",
    "            \"L2-L3\": \"4\",\n",
    "            \"L1-L2\": \"5\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    radiomic_features = pd.read_csv(\n",
    "        root_dir.joinpath(\"data\", f\"filtered_midas900_{from_image}_radiomics.csv\"),\n",
    "        sep=\",\",\n",
    "    )\n",
    "    radiomic_features.rename(columns={\"Unnamed: 0\": \"ID\"}, inplace=True)\n",
    "\n",
    "    return id_labels.merge(radiomic_features, on=\"ID\")\n",
    "\n",
    "\n",
    "def get_labels_and_features(\n",
    "    rater: str = \"900\", label: int = 1, from_image: str = \"t2w\"\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the given label and returns the labels and features as separate dataframes.\n",
    "\n",
    "    :param rater: The rater identifier. Default is \"900\".\n",
    "    :type rater: str\n",
    "    :param label: A number from 1 to 5 indicating the disc of interest. Default is 1.\n",
    "    :type label: bool\n",
    "    :return: A tuple containing the labels and features.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "\n",
    "    data = build_dataframe_from_csv(rater=rater, from_image=from_image)\n",
    "\n",
    "    data = data.rename(columns={str(label): f\"label{label}\", \"ID\": f\"label{label}ID\"})\n",
    "    columns_mask = data.columns.str.contains(\n",
    "        f\"label{label}\"\n",
    "    ) & ~data.columns.str.contains(\"Configuration\")\n",
    "    data = data.loc[:, columns_mask]\n",
    "    data = data.rename(columns={f\"label{label}\": \"label\", f\"label{label}ID\": \"ID\"})\n",
    "\n",
    "    label_data = data.dropna(axis=0, how=\"any\")\n",
    "    label_data = label_data.loc[label_data[\"label\"] != 0]\n",
    "    label_data[\"ID\"] = label_data[\"ID\"].map(lambda x: x + str(label))\n",
    "    label_data = label_data.set_index(\"ID\")\n",
    "    labels = label_data[\"label\"]\n",
    "    features = label_data[\n",
    "        label_data.select_dtypes(include=\"number\").columns.tolist()\n",
    "    ].drop(columns=\"label\")\n",
    "    return labels, features\n",
    "\n",
    "\n",
    "def get_labels_and_features_all_discs(\n",
    "    rater: str = \"900\", verbose: bool = False, from_image: str = \"t1w_t2w\"\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Get labels and features for all discs.\n",
    "\n",
    "    :param rater: The rater identifier. Default is \"900\".\n",
    "    :type rater: str\n",
    "    :param verbose: Whether to print additional information and plot the label distribution. Default is False.\n",
    "    :type verbose: bool\n",
    "    :return: A tuple containing the labels and features.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    for label in range(1, 6):\n",
    "        labels_i, features_i = get_labels_and_features(\n",
    "            rater=rater, label=label, from_image=from_image\n",
    "        )\n",
    "        labels.append(labels_i)\n",
    "        features_i = features_i.rename(\n",
    "            columns={\n",
    "                name: name.replace(f\"label{label}_\", \"\")\n",
    "                for name in features_i.columns.to_list()\n",
    "            }\n",
    "        )\n",
    "        features.append(features_i)\n",
    "    features = pd.concat(features, axis=0)\n",
    "    labels = pd.concat(labels, axis=0)\n",
    "    if verbose:\n",
    "        print(f\"Labels shape: {labels.shape}, Features shape: {features.shape}\")\n",
    "        labels.plot(kind=\"hist\", xticks=[1, 2, 3, 4, 5], title=\"Label distribution\")\n",
    "        plt.show()\n",
    "    return labels, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "class VarianceFeatureReduction(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    VarianceFeatureReduction is a transformer that reduces the feature space by removing features with low variance.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    threshold : float, optional (default=0.05)\n",
    "        The threshold below which features will be removed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threshold=0.05):\n",
    "        self.threshold = threshold\n",
    "        self.selector = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the VarianceFeatureReduction transformer to the input data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like, shape (n_samples,), optional (default=None)\n",
    "            The target values.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        self.selector = VarianceThreshold(threshold=self.threshold)\n",
    "        self.selector.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Transform the input data by removing features with low variance.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like, shape (n_samples,), optional (default=None)\n",
    "            The target values.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        X_ : array-like, shape (n_samples, n_selected_features)\n",
    "            The transformed data with low variance features removed.\n",
    "        \"\"\"\n",
    "        X_ = X.copy()\n",
    "        X_ = X_.loc[:, self.selector.get_support()]\n",
    "        return X_\n",
    "\n",
    "\n",
    "class CorrelationFeatureReduction(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer class for reducing features based on correlation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    threshold : float, optional (default=0.8)\n",
    "        The threshold above which features will be removed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threshold=0.8):\n",
    "        self.threshold = threshold\n",
    "        self.corr_matrix_var = None\n",
    "        self.to_keep = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer to the input data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas DataFrame\n",
    "            The input data.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : CorrelationFeatureReduction\n",
    "            The fitted transformer object.\n",
    "\n",
    "        \"\"\"\n",
    "        self.corr_matrix_var = X.corr(\n",
    "            method=\"spearman\"\n",
    "        ).abs()  # absolute correlation matrix\n",
    "\n",
    "        # Initialize the flag vector with True values\n",
    "        self.to_keep = np.full((self.corr_matrix_var.shape[1]), True, dtype=bool)\n",
    "\n",
    "        for i in range(self.corr_matrix_var.shape[1]):\n",
    "            for j in range(i + 1, self.corr_matrix_var.shape[1]):\n",
    "                if (\n",
    "                    self.to_keep[i]\n",
    "                    and self.corr_matrix_var.iloc[i, j] >= self.threshold\n",
    "                ):\n",
    "                    if self.to_keep[j]:\n",
    "                        self.to_keep[j] = False\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Transform the input data by removing highly correlated features.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas DataFrame\n",
    "            The input data.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        X_ : pandas DataFrame\n",
    "            The transformed data with highly correlated features removed.\n",
    "\n",
    "        \"\"\"\n",
    "        X_ = X.copy()\n",
    "        X_ = X_.iloc[:, self.to_keep]\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier,\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def test_multiple_models(features, labels):\n",
    "    # Define classifiers to test\n",
    "    classifiers = {\n",
    "        \"Random Forest\": RandomForestClassifier(),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "        \"SVM\": SVC(),\n",
    "        \"Logistic Regression\": LogisticRegression(),\n",
    "        \"Stochastic Gradient Descent\": SGDClassifier(),\n",
    "        \"Naive Bayes\": GaussianNB(),\n",
    "        \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "        \"Multilayer Perceptron\": MLPClassifier(),\n",
    "        \"AdaBoost\": AdaBoostClassifier(),\n",
    "        \"ExtraTrees\": ExtraTreesClassifier(),\n",
    "    }\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.25, random_state=0, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Test each classifier\n",
    "    f1_scores = {}\n",
    "    for name, clf in classifiers.items():\n",
    "        pipeline = Pipeline(\n",
    "            [\n",
    "                (\"variancethreshold\", VarianceFeatureReduction(threshold=0.05)),\n",
    "                (\"correlationreduction\", CorrelationFeatureReduction()),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"classifier\", clf),\n",
    "            ]\n",
    "        )\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        f1_scores[name] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    # Select the classifier with the highest F1 score\n",
    "    best_classifier = max(f1_scores, key=f1_scores.get)  # type: ignore\n",
    "    print(\"Best classifier:\", best_classifier)\n",
    "    print(\"F1 score:\", f1_scores[best_classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "\n",
    "def cv(clf, features, labels):\n",
    "    # Create a stratified 5-fold cross-validation object\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    pipeline_clf = Pipeline(\n",
    "        [\n",
    "            (\"variancethreshold\", VarianceFeatureReduction(threshold=0.05)),\n",
    "            (\"correlationreduction\", CorrelationFeatureReduction()),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"classifier\", clf),\n",
    "        ]\n",
    "    )\n",
    "    scores = cross_val_score(\n",
    "        pipeline_clf, features, labels, cv=skf, scoring=\"f1_weighted\"\n",
    "    )\n",
    "    print(f\"Cross Validation F1 Score: {scores.mean():0.4f} +/- {scores.std():0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "def imbalanced_learning_suite(features, labels):\n",
    "    # Define classifiers to test\n",
    "    classifiers = {\n",
    "        \"Balanced Bagging Classifier\": BalancedBaggingClassifier(\n",
    "            sampler=RandomUnderSampler()\n",
    "        ),\n",
    "        \"Balanced RandomForest Classifier\": BalancedRandomForestClassifier(),\n",
    "        \"RUS Boost Classifier\": RUSBoostClassifier(),\n",
    "        \"Easy Ensemble Classifier\": EasyEnsembleClassifier(),\n",
    "    }\n",
    "\n",
    "    # Create a stratified 5-fold cross-validation object\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for name, clf in classifiers.items():\n",
    "        pipeline_clf = Pipeline(\n",
    "            [\n",
    "                (\"variancethreshold\", VarianceFeatureReduction(threshold=0.05)),\n",
    "                (\"correlationreduction\", CorrelationFeatureReduction()),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"classifier\", clf),\n",
    "            ]\n",
    "        )\n",
    "        scores = cross_val_score(\n",
    "            pipeline_clf, features, labels, cv=skf, scoring=\"f1_weighted\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{name}: {scores.mean():0.2f} f1 with a standard deviation of {scores.std():0.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yellowbrick.classifier as viz\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "from yellowbrick.style import set_palette\n",
    "\n",
    "set_palette(colors)\n",
    "\n",
    "\n",
    "def visual_metrics(clf, features, labels, classes=[\"1\", \"2\", \"3\", \"4\", \"5\"]):\n",
    "    labels_ = labels.copy()\n",
    "    if min(labels_) != 0:\n",
    "        labels_ = labels_ - min(labels_)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels_, test_size=0.25, random_state=0, stratify=labels_\n",
    "    )\n",
    "\n",
    "    _, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    labels.plot(\n",
    "        kind=\"hist\",\n",
    "        title=\"Pfirmann grade distribution\",\n",
    "        ax=axes[0],\n",
    "        xticks=[1, 2, 3, 4, 5],\n",
    "        align=\"mid\",\n",
    "    )\n",
    "\n",
    "    pipeline_clf = Pipeline(\n",
    "        [\n",
    "            (\"variancethreshold\", VarianceFeatureReduction(threshold=0.05)),\n",
    "            (\"correlationreduction\", CorrelationFeatureReduction()),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"classifier\", clf),\n",
    "        ]\n",
    "    )\n",
    "    pipeline_clf.fit(X_train, y_train)\n",
    "    axes[1].set_title(\"Classification Report\")\n",
    "    axes[1].set_ylabel(\"Class\")\n",
    "    visualizer_class = viz.ClassificationReport(\n",
    "        pipeline_clf, classes=classes[::-1], support=True, ax=axes[1], cmap=\"Blues\"\n",
    "    )\n",
    "    visualizer_class.score(X_test, y_test)\n",
    "\n",
    "    axes[2].set_title(\"Classification Prediction Error\")\n",
    "    axes[2].set_xlabel(\"Class\")\n",
    "    axes[2].set_ylabel(\"Number of Predictions\")\n",
    "    visualizer_pred = viz.ClassPredictionError(\n",
    "        pipeline_clf, classes=classes, ax=axes[2]\n",
    "    )\n",
    "    visualizer_pred.score(X_test, y_test)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    predictions = pipeline_clf.predict(X_test)\n",
    "    print(f\"Accuracy within one grade: {accuracy_within_one(y_test, predictions):0.2f}\")\n",
    "    print(f\"Balanced accuracy: {balanced_accuracy_score(y_test, predictions):0.2f}\")\n",
    "    print(classification_report(y_test, predictions, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "def random_search(clf, distribution, features, labels):\n",
    "    pipeline_clf = Pipeline(\n",
    "        [\n",
    "            (\"variancethreshold\", VarianceFeatureReduction(threshold=0.05)),\n",
    "            (\"correlationreduction\", CorrelationFeatureReduction()),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"classifier\", clf),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    rs_clf = RandomizedSearchCV(\n",
    "        pipeline_clf,\n",
    "        distribution,\n",
    "        cv=skf,\n",
    "        scoring=\"f1_weighted\",\n",
    "        n_iter=10,\n",
    "        random_state=0,\n",
    "    )\n",
    "    search = rs_clf.fit(features, labels)\n",
    "\n",
    "    print(f\"Best parameter (CV score={search.best_score_:0.3f}): {search.best_params_}\")\n",
    "    return {\n",
    "        key.replace(\"classifier__\", \"\"): value\n",
    "        for key, value in search.best_params_.items()\n",
    "        if key.startswith(\"classifier__\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkMaskVol(image, mask, label):\n",
    "    try:\n",
    "        imageoperations.checkMask(\n",
    "            image, mask, minimumROIDimensions=3, minimumROISize=1000, label=label\n",
    "        )\n",
    "        result = label\n",
    "    except Exception as e:\n",
    "        result = None\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_within_one(labels, predictions):\n",
    "    # Calculate the absolute difference between labels and predictions\n",
    "    diff = abs(labels - predictions)\n",
    "    # Count the number of differences that are less than or equal to one\n",
    "    correct_predictions = sum(diff <= 1)\n",
    "    # Calculate the accuracy\n",
    "    accuracy = correct_predictions / len(labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midas_img_relation = pd.read_csv(\n",
    "    root_dir.joinpath(\"data\", \"filtered_midas900_t2w.csv\"), sep=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "devices = []\n",
    "for _, row in midas_img_relation.iterrows():\n",
    "    img_path = pathlib.Path(row[\"Image\"])\n",
    "    metadata_path = img_path.with_suffix(\"\").with_suffix(\".json\")\n",
    "    with open(metadata_path, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "        manufacturer = metadata.get(\"00080070\", {}).get(\"Value\", [\"N/A\"])[0]\n",
    "        model = metadata.get(\"00081090\", {}).get(\"Value\", [\"N/A\"])[0]\n",
    "        field_strength = metadata.get(\"00180087\", {}).get(\"Value\", [\"N/A\"])[0]\n",
    "    devices.append(\n",
    "        {\n",
    "            \"Manufacturer\": manufacturer,\n",
    "            \"Model name\": model,\n",
    "            \"Field Strength\": field_strength,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Manufacturer\").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = build_dataframe_from_csv(rater=\"JDCarlos\", from_image=\"t2w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "discs = {\n",
    "    \"1\": \"L5-S\",\n",
    "    \"2\": \"L4-L5\",\n",
    "    \"3\": \"L3-L4\",\n",
    "    \"4\": \"L2-L3\",\n",
    "    \"5\": \"L1-L2\",\n",
    "}\n",
    "for i in range(1, 6):\n",
    "    s = df[f\"{i}\"].value_counts()\n",
    "    s.name = discs[f\"{i}\"]\n",
    "    a.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(a).T\n",
    "\n",
    "fig = px.bar(\n",
    "    df1, title=\"Pfirrmann Grade Distribution\", color_discrete_sequence=colors\n",
    ")  # replace 0 with your column name if needed\n",
    "total_count = df1.sum(axis=1)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df1.index,\n",
    "        y=total_count,\n",
    "        mode=\"text\",\n",
    "        text=total_count,\n",
    "        textposition=\"top center\",\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "fig.update_traces(textfont_size=12)\n",
    "fig.update_xaxes(title_text=\"Pfirrmann Grade\")\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Frequency\", showgrid=True, gridcolor=\"rgba(184, 184, 184, 0.3)\"\n",
    ")\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "    paper_bgcolor=\"rgba(0,0,0,0)\",\n",
    "    legend_title_text=\"Intervertebral Disc\",\n",
    "    grid_rows=1,\n",
    ")\n",
    "fig.show()\n",
    "# pio.write_image(fig, root_dir.joinpath(\"figures\", \"pfirrmann_grade_distribution.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai.transforms as transforms\n",
    "import torch\n",
    "\n",
    "\n",
    "class CheckMaskVol(transforms.MapTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys=[\"image\", \"mask\"],\n",
    "        minimum_roi_dimensions: int = 3,\n",
    "        minimum_roi_size: int = 1000,\n",
    "    ):\n",
    "        super().__init__(keys)\n",
    "        self.minimum_roi_dimensions = minimum_roi_dimensions\n",
    "        self.minimum_roi_size = minimum_roi_size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        image = sitk.ReadImage(x[self.keys[0]])\n",
    "        mask = sitk.ReadImage(x[self.keys[1]])\n",
    "        labels = np.unique(sitk.GetArrayFromImage(mask).ravel())\n",
    "        valid_labels = []\n",
    "        for label in labels:\n",
    "            if label != 0:\n",
    "                try:\n",
    "                    imageoperations.checkMask(\n",
    "                        image,\n",
    "                        mask,\n",
    "                        minimumROIDimensions=self.minimum_roi_dimensions,\n",
    "                        minimumROISize=self.minimum_roi_size,\n",
    "                        label=label,\n",
    "                    )\n",
    "                    result = label\n",
    "                except Exception as e:\n",
    "                    result = None\n",
    "                if result:\n",
    "                    valid_labels.append(result)\n",
    "        x[\"valid_labels\"] = valid_labels[:5]\n",
    "        return x\n",
    "\n",
    "\n",
    "class CropForegroundd(transforms.MapTransform):\n",
    "    def __init__(\n",
    "        self, keys=[\"image\"], source_key=\"mask\", margin=0, k_divisible=(64, 64, 1)\n",
    "    ):\n",
    "        super().__init__(keys)\n",
    "        self.k_divisible = k_divisible\n",
    "        self.margin = margin\n",
    "        self.source_key = source_key\n",
    "\n",
    "    def __call__(self, x):\n",
    "        key = self.keys[0]\n",
    "        bool_mask = torch.where(\n",
    "            x[self.source_key] == x[\"valid_labels\"][0], x[self.source_key], 0\n",
    "        )\n",
    "        for label in x[\"valid_labels\"][1:]:\n",
    "            bool_mask += torch.where(x[self.source_key] == label, x[self.source_key], 0)\n",
    "        input_data = {\"image\": x[key] * bool_mask, \"mask\": x[self.source_key]}\n",
    "        discs = []\n",
    "        labels = []\n",
    "        for label, disc in enumerate(x[\"valid_labels\"], start=1):\n",
    "            select_fn = lambda x: x == disc\n",
    "            crop = transforms.CropForegroundd(\n",
    "                keys=self.keys,\n",
    "                source_key=self.source_key,\n",
    "                select_fn=select_fn,\n",
    "                margin=self.margin,\n",
    "                k_divisible=self.k_divisible,\n",
    "            )(input_data)\n",
    "            crop2 = transforms.CenterSpatialCropd(keys=[\"image\"], roi_size=(-1, -1, 1))(\n",
    "                crop\n",
    "            )\n",
    "            discs.append(crop2[\"image\"])\n",
    "            labels.append(x[str(label)])\n",
    "        return [{\"image\": disc, \"label\": label} for disc, label in zip(discs, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_ = transforms.Compose(\n",
    "    [\n",
    "        CheckMaskVol(\n",
    "            keys=[\"image\", \"mask\"], minimum_roi_dimensions=3, minimum_roi_size=1000\n",
    "        ),\n",
    "        transforms.LoadImaged(\n",
    "            keys=[\"image\", \"mask\"], image_only=True, ensure_channel_first=True\n",
    "        ),\n",
    "        CropForegroundd(\n",
    "            keys=[\"image\"], source_key=\"mask\", margin=0, k_divisible=(1, 1, 1)\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"Discs\": [], \"Pfirmann\": [], \"Array\": []}\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    data = {\n",
    "        \"image\": row[\"Image\"],\n",
    "        \"mask\": row[\"Mask\"],\n",
    "        \"1\": row[\"1\"],\n",
    "        \"2\": row[\"2\"],\n",
    "        \"3\": row[\"3\"],\n",
    "        \"4\": row[\"4\"],\n",
    "        \"5\": row[\"5\"],\n",
    "    }\n",
    "    result_row = transforms_(data)\n",
    "    for idx, result in enumerate(result_row, start=1):\n",
    "        results[\"Discs\"].append(idx)\n",
    "        results[\"Pfirmann\"].append(result[\"label\"])\n",
    "        results[\"Array\"].append(result[\"image\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms_df = pd.DataFrame(results)\n",
    "histograms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = histograms_df.copy()\n",
    "process[\"Normalized Array\"] = process[\"Array\"].map(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min())\n",
    ")\n",
    "process[\"Histogram\"] = process[\"Normalized Array\"].map(\n",
    "    lambda x: np.histogram(x, bins=10)[0]\n",
    ")\n",
    "grouped_by_disc = process.groupby(\"Pfirmann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = np.arange(0, 1.01, 1 / 10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(4, -1, -1):\n",
    "    plt.bar(\n",
    "        x=x_[:-1],\n",
    "        height=grouped_by_disc[\"Histogram\"].mean().iloc[i],\n",
    "        width=np.diff(x_),\n",
    "        label=str(i + 1),\n",
    "    )\n",
    "plt.legend()\n",
    "# plt.ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# X, Y positions for the bars\n",
    "x_positions = x_[:-1]\n",
    "y_positions = [0, 1, 2, 3, 4]  # Different datasets on different y-positions\n",
    "\n",
    "# Width of each bar and yz-space between bars\n",
    "width = np.diff(x_)\n",
    "y_space = 0.8\n",
    "\n",
    "# Adding the histograms to the plot\n",
    "for i, hist in enumerate(grouped_by_disc[\"Histogram\"].mean()):\n",
    "    ax.bar(\n",
    "        x_positions,\n",
    "        hist,\n",
    "        zs=y_positions[i],\n",
    "        zdir=\"y\",\n",
    "        width=width,\n",
    "        align=\"center\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"X-axis (Value)\")\n",
    "# ax.set_ylabel('Y-axis (Dataset)')\n",
    "ax.set_zlabel(\"Z-axis (Frequency)\")\n",
    "\n",
    "# Setting the y-ticks to correspond to different datasets\n",
    "ax.set_yticks(y_positions)\n",
    "ax.set_yticklabels(\n",
    "    [\"Pfirrmann 1\", \"Pfirrmann 2\", \"Pfirrmann 3\", \"Pfirrmann 4\", \"Pfirrmann 5\"]\n",
    ")\n",
    "\n",
    "plt.title(\"3D Histograms\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intraclass correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ICC_feature_reduction(\n",
    "    input_dataframe,\n",
    "    input_dataframe_ICC1,\n",
    "    input_dataframe_ICC2,\n",
    "    input_dataframe_ICC3,\n",
    "    ICC_thresh,\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pingouin as pg\n",
    "    import numpy as np\n",
    "\n",
    "    # Initialize an empty flag vector\n",
    "    to_keep = []\n",
    "    # Iterate over features\n",
    "    for feature in input_dataframe.columns:\n",
    "        # Concatenate feature values vertically\n",
    "        feature_data = pd.concat(\n",
    "            [\n",
    "                input_dataframe[[feature]],\n",
    "                input_dataframe_ICC1[[feature]],\n",
    "                input_dataframe_ICC2[[feature]],\n",
    "                input_dataframe_ICC3[[feature]],\n",
    "            ],\n",
    "            axis=0,\n",
    "            ignore_index=False,\n",
    "        )\n",
    "\n",
    "        # Append patient/repetition information\n",
    "        # Create a repetition/patients column\n",
    "        result_array = np.repeat(\n",
    "            [1, 2, 3, 4],\n",
    "            [\n",
    "                len(input_dataframe),\n",
    "                len(input_dataframe),\n",
    "                len(input_dataframe),\n",
    "                len(input_dataframe),\n",
    "            ],\n",
    "        )\n",
    "        feature_data[\"Repetition\"] = result_array\n",
    "        feature_data[\"Patients\"] = pd.factorize(feature_data.index)[0]\n",
    "        feature_data = feature_data.rename(columns={feature: \"FeatureValue\"})\n",
    "\n",
    "        # Compute ICC\n",
    "        icc_result = pg.intraclass_corr(\n",
    "            data=feature_data,\n",
    "            targets=\"Patients\",\n",
    "            raters=\"Repetition\",\n",
    "            ratings=\"FeatureValue\",\n",
    "        )\n",
    "        # Extract ICC value\n",
    "        icc_value = icc_result[\"ICC\"].iloc[\n",
    "            1\n",
    "        ]  # ICC2: A random sample of raters rate each target. Measure of absolute agreement.\n",
    "\n",
    "        # Check if ICC is greater than the threshold\n",
    "        if icc_value > ICC_thresh:\n",
    "            to_keep.append(True)\n",
    "        else:\n",
    "            to_keep.append(False)\n",
    "\n",
    "    return input_dataframe.loc[:, to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ICC_reduced_per_disc(disc: int):\n",
    "    t2 = pd.read_csv(\n",
    "        root_dir.joinpath(\"data\", f\"filtered_midas900_t2w_radiomics.csv\"), sep=\",\"\n",
    "    )\n",
    "    t2 = t2.loc[:, t2.columns.str.contains(f\"label{disc}\")]\n",
    "    t2 = t2[t2.select_dtypes(include=\"number\").columns.tolist()]\n",
    "\n",
    "    t2_shifted = pd.read_csv(\n",
    "        root_dir.joinpath(\n",
    "            \"data\", \"mask_perturbation\", f\"filtered_midas900_t2w_radiomics_shifted.csv\"\n",
    "        ),\n",
    "        sep=\",\",\n",
    "    )\n",
    "    t2_shifted = t2_shifted.loc[:, t2_shifted.columns.str.contains(f\"label{disc}\")]\n",
    "    t2_shifted = t2_shifted[t2_shifted.select_dtypes(include=\"number\").columns.tolist()]\n",
    "\n",
    "    t2_eroded = pd.read_csv(\n",
    "        root_dir.joinpath(\n",
    "            \"data\", \"mask_perturbation\", f\"filtered_midas900_t2w_radiomics_eroded.csv\"\n",
    "        ),\n",
    "        sep=\",\",\n",
    "    )\n",
    "    t2_eroded = t2_eroded.loc[:, t2_eroded.columns.str.contains(f\"label{disc}\")]\n",
    "    t2_eroded = t2_eroded[t2_eroded.select_dtypes(include=\"number\").columns.tolist()]\n",
    "\n",
    "    t2_dilated = pd.read_csv(\n",
    "        root_dir.joinpath(\n",
    "            \"data\", \"mask_perturbation\", f\"filtered_midas900_t2w_radiomics_dilated.csv\"\n",
    "        ),\n",
    "        sep=\",\",\n",
    "    )\n",
    "    t2_dilated = t2_dilated.loc[:, t2_dilated.columns.str.contains(f\"label{disc}\")]\n",
    "    t2_dilated = t2_dilated[t2_dilated.select_dtypes(include=\"number\").columns.tolist()]\n",
    "\n",
    "    if disc == 1:\n",
    "        t2.drop(index=317, inplace=True)\n",
    "        t2_shifted.drop(index=317, inplace=True)\n",
    "        t2_eroded.drop(index=317, inplace=True)\n",
    "        t2_dilated.drop(index=317, inplace=True)\n",
    "\n",
    "    t2_reduced_ICC = ICC_feature_reduction(t2, t2_shifted, t2_eroded, t2_dilated, 0.85)\n",
    "    print(f\"t1.     {t2_reduced_ICC.shape[1]} features retained from {t2.shape[1]}\")\n",
    "\n",
    "    return t2_reduced_ICC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_1_t2_reduced_ICC = compute_ICC_reduced_per_disc(disc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_2_t2_reduced_ICC = compute_ICC_reduced_per_disc(disc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_3_t2_reduced_ICC = compute_ICC_reduced_per_disc(disc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_4_t2_reduced_ICC = compute_ICC_reduced_per_disc(disc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_5_t2_reduced_ICC = compute_ICC_reduced_per_disc(disc=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        disc_1_t2_reduced_ICC,\n",
    "        disc_2_t2_reduced_ICC,\n",
    "        disc_3_t2_reduced_ICC,\n",
    "        disc_4_t2_reduced_ICC,\n",
    "        disc_5_t2_reduced_ICC,\n",
    "    ],\n",
    "    axis=1,\n",
    ").to_csv(\n",
    "    root_dir.joinpath(\n",
    "        \"data\",\n",
    "        \"mask_perturbation\",\n",
    "        f\"filtered_midas900_t2w_radiomics_reduced_ICC_per_disc.csv\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Near-zero variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Feature reduction based on variance thresholding (remove features with variance smaller than 0.05)\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Initialize selector based on VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.05)\n",
    "\n",
    "#  Estimate variances and reduce features\n",
    "labels, radiomic_features = get_labels_and_features_all_discs(rater=\"JDCarlos\")\n",
    "selector.fit_transform(radiomic_features)\n",
    "\n",
    "# Get the selected feature labels and reduce the Radiomic_Feature dataframe\n",
    "radiomic_features_var = radiomic_features.loc[:, selector.get_support()]\n",
    "\n",
    "# Display the number of features removed\n",
    "print(\n",
    "    f\"{np.count_nonzero(~selector.get_support())}/{radiomic_features.shape[1]} features were removed due to near-zero variance.\"\n",
    ")\n",
    "\n",
    "del selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_var = radiomic_features_var.corr(\n",
    "    method=\"spearman\"\n",
    ").abs()  # absolute correlation matrix\n",
    "\n",
    "# Initialize the flag vector with True values\n",
    "to_keep = np.full((corr_matrix_var.shape[1]), True, dtype=bool)\n",
    "\n",
    "for i in range(corr_matrix_var.shape[1]):\n",
    "    for j in range(i + 1, corr_matrix_var.shape[1]):\n",
    "        if to_keep[i] and corr_matrix_var.iloc[i, j] >= 0.8:\n",
    "            if to_keep[j]:\n",
    "                to_keep[j] = False\n",
    "\n",
    "# Retain features that are not higly correlated\n",
    "radiomic_features_corr = radiomic_features_var.iloc[:, to_keep]\n",
    "\n",
    "print(\n",
    "    f\"{np.count_nonzero(~to_keep)}/{radiomic_features_var.shape[1]} features were removed due to high correlation. {radiomic_features_corr.shape[1]} features remaining.\"\n",
    ")\n",
    "\n",
    "del to_keep, i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Calculate the correlation matrix of the original feature set\n",
    "corr_matrix = radiomic_features.corr(method=\"spearman\")\n",
    "# Display the correlation matrix\n",
    "plt.figure(figsize=(8, 6.5))\n",
    "sns.heatmap(corr_matrix, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation of Radiomic features\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate the correlation matrix of the reduced feature set\n",
    "corr_matrix_red = radiomic_features_corr.corr(method=\"spearman\")\n",
    "# Display the correlation matrix\n",
    "plt.figure(figsize=(8, 6.5))\n",
    "sns.heatmap(corr_matrix_red, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation of reduced radiomic features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from scipy.stats import zscore\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "radiomic_features_clus = copy.deepcopy(radiomic_features_corr)\n",
    "# Normalize the data\n",
    "radiomic_features_clus = zscore(radiomic_features_clus, axis=0)\n",
    "\n",
    "# Calculate and plot the clustergram\n",
    "row_linkage = hierarchy.linkage(\n",
    "    distance.pdist(radiomic_features_clus.to_numpy()), method=\"ward\"\n",
    ")\n",
    "col_linkage = hierarchy.linkage(\n",
    "    distance.pdist(radiomic_features_clus.T.to_numpy()), method=\"ward\"\n",
    ")\n",
    "g = sns.clustermap(\n",
    "    radiomic_features_clus,\n",
    "    row_linkage=row_linkage,\n",
    "    col_linkage=col_linkage,\n",
    "    method=\"ward\",\n",
    "    vmin=-3,\n",
    "    vmax=3,\n",
    "    figsize=(8, 10),\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "g.ax_cbar.set_position((0.90, 0.2, 0.03, 0.3))\n",
    "\n",
    "# Extract 5 disc degeneration clusters and append the \"Clusters\" variable to the DataFrame\n",
    "n_clusters = 5\n",
    "radiomic_features_clus[\"Clusters\"] = fcluster(\n",
    "    row_linkage, n_clusters, criterion=\"maxclust\"\n",
    ")\n",
    "\n",
    "# Print the cluster assignments\n",
    "print(\"Cluster Assignments:\", radiomic_features_clus[\"Clusters\"])\n",
    "\n",
    "del g, n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Concatenate the target clinical variable to the radiomic DataFrame\n",
    "radiomic_features_clus = pd.concat([radiomic_features_clus, labels], axis=1)\n",
    "\n",
    "# Barplot clusters/grades\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.countplot(\n",
    "    x=\"Clusters\", hue=\"label\", data=radiomic_features_clus, palette=\"coolwarm\"\n",
    ")\n",
    "plt.title(\"Distribution of Pfirmann grade in each Cluster\")\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Pfirmann grade\", loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "# Perform chi-squared test\n",
    "chi2, p, _, _ = chi2_contingency(\n",
    "    pd.crosstab(radiomic_features_clus[\"label\"], radiomic_features_clus[\"Clusters\"])\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Chi-squared statistic: {chi2}\")\n",
    "print(f\"P-value: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_results(clf, disc: int = 1):\n",
    "    labels, features = get_labels_and_features(rater=\"JDCarlos\", label=disc)\n",
    "    cv(clf, features, labels)\n",
    "    visual_metrics(clf, features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in range(1, 6):\n",
    "    print(f\"Disc: {label}\")\n",
    "    labels, features = get_labels_and_features(rater=\"JDCarlos\", label=label)\n",
    "    test_multiple_models(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Disc 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "disc_results(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disc 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "disc_results(clf, disc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disc 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier()\n",
    "disc_results(clf, disc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disc 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "disc_results(clf, disc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disc 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "disc_results(clf, disc=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All discs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_discs_results(clf):\n",
    "    labels, features = get_labels_and_features_all_discs(rater=\"JDCarlos\")\n",
    "    # cv(clf, features, labels)\n",
    "    visual_metrics(clf, features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, features = get_labels_and_features_all_discs(rater=\"JDCarlos\")\n",
    "test_multiple_models(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "all_discs_results(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, features = get_labels_and_features_all_discs(rater=\"JDCarlos\", from_image=\"t2w\")\n",
    "labels_ = labels.copy()\n",
    "if min(labels_) != 0:\n",
    "    labels_ = labels_ - min(labels_)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels_, test_size=0.25, random_state=0, stratify=labels_\n",
    ")\n",
    "\n",
    "clf = ExtraTreesClassifier()\n",
    "pipeline_clf = Pipeline(\n",
    "    [\n",
    "        (\"variancethreshold\", VarianceFeatureReduction(threshold=0.05)),\n",
    "        (\"correlationreduction\", CorrelationFeatureReduction()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"classifier\", clf),\n",
    "    ]\n",
    ")\n",
    "pipeline_clf.fit(X_train, y_train)\n",
    "# Predict the values for the test set\n",
    "y_pred = pipeline_clf.predict(X_test)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Convert the report to a DataFrame\n",
    "df = pd.DataFrame(report).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "r = permutation_importance(pipeline_clf, X_test, y_test, n_repeats=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "        print(\n",
    "            f\"{list(X_test.columns)[i]:<8} \"\n",
    "            f\"{r.importances_mean[i]:.3f}\"\n",
    "            f\" +/- {r.importances_std[i]:.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_int = lambda x: str(int(x)) if x > 1 else f\"{x:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.iloc[:5]\n",
    "df1_ = df1.copy()\n",
    "df1_[\"support\"] = 0.1\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=df1_.values.tolist(),\n",
    "        x=df1.columns,\n",
    "        y=[str(i) for i in range(1, 6)],\n",
    "        colorscale=[\"#ffffff\", colors[1]],\n",
    "        showscale=True,\n",
    "        xgap=1,\n",
    "        ygap=1,\n",
    "        zmin=0,\n",
    "        zmax=1,\n",
    "        hoverinfo=\"none\",\n",
    "        hoverongaps=False,\n",
    "        text=[[float_int(val) for val in row] for row in df1.values.tolist()],\n",
    "        texttemplate=\"%{text}\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    margin=dict(pad=10),\n",
    "    plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "    legend_title_text=\"Intervertebral Disc\",\n",
    "    grid_rows=1,\n",
    "    title=\"Per grade classification results for the five level grading task\",\n",
    ")\n",
    "fig.update_xaxes(\n",
    "    automargin=True,\n",
    "    tickvals=list(range(len(df1.columns))),\n",
    "    ticktext=[col.capitalize() for col in df1.columns],\n",
    "    showgrid=False,\n",
    ")\n",
    "fig.update_yaxes(automargin=True, title_text=\"Pfirrmann Grade\", showgrid=False)\n",
    "fig.update_traces(\n",
    "    textfont_size=16,\n",
    ")\n",
    "fig.show()\n",
    "# fig.write_image(root_dir.joinpath(\"figures\", \"per_grade_5_levels.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_series = pd.Series(y_pred, name=\"Predicted\")\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([pred_series, y_test]).add(1).astype(int)\n",
    "df = df.T.groupby(\"Predicted\").value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    df, title=\"Class Prediction Error\", color_discrete_sequence=colors\n",
    ")  # replace 0 with your column name if needed\n",
    "\n",
    "total_count = df.sum(axis=1)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df.index,\n",
    "        y=total_count,\n",
    "        mode=\"text\",\n",
    "        text=total_count,\n",
    "        textposition=\"top center\",\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "fig.update_traces(textfont_size=12)\n",
    "fig.update_xaxes(title_text=\"Predicted Pfirrmann Grade\")\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Frequency\", showgrid=True, gridcolor=\"rgba(184, 184, 184, 0.3)\"\n",
    ")\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "    legend_title_text=\"True Pfirrmann Grade\",\n",
    "    grid_rows=1,\n",
    ")\n",
    "fig.show()\n",
    "# fig.write_image(root_dir.joinpath(\"figures\", \"class_prediction_error_5level.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, features = get_labels_and_features_all_discs(rater=\"JDCarlos\")\n",
    "imbalanced_learning_suite(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining classes 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_results_combining_1_and_2(clf, disc: int = 1):\n",
    "    labels, features = get_labels_and_features(rater=\"JDCarlos\", label=disc)\n",
    "    labels.loc[labels == 1] = 2\n",
    "    cv(clf, features, labels)\n",
    "    visual_metrics(clf, features, labels, classes=[\"1 and 2\", \"3\", \"4\", \"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in range(1, 6):\n",
    "    print(f\"Disc: {label}\")\n",
    "    labels, features = get_labels_and_features(rater=\"JDCarlos\", label=label)\n",
    "    labels.loc[labels == 1] = 2\n",
    "    test_multiple_models(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disc 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier()\n",
    "disc_results_combining_1_and_2(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disc 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier()\n",
    "disc_results_combining_1_and_2(clf, disc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disc 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier()\n",
    "disc_results_combining_1_and_2(clf, disc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disc 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "disc_results_combining_1_and_2(clf, disc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disc 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "disc_results_combining_1_and_2(clf, disc=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All discs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_discs_results_combining_1_and_2(clf):\n",
    "    labels, features = get_labels_and_features_all_discs(rater=\"JDCarlos\")\n",
    "    labels.loc[labels == 1] = 2\n",
    "    cv(clf, features, labels)\n",
    "    visual_metrics(clf, features, labels, classes=[\"1 and 2\", \"3\", \"4\", \"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, features = get_labels_and_features_all_discs(rater=\"JDCarlos\")\n",
    "labels.loc[labels == 1] = 2\n",
    "test_multiple_models(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "all_discs_results_combining_1_and_2(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, features = get_labels_and_features_all_discs(rater=\"JDCarlos\")\n",
    "labels.loc[labels == 1] = 2\n",
    "imbalanced_learning_suite(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, features = get_labels_and_features_all_discs(rater=\"JDCarlos\", from_image=\"t2w\")\n",
    "labels_ = labels.copy()\n",
    "labels_.loc[labels_ == 1] = 2\n",
    "if min(labels_) != 0:\n",
    "    labels_ = labels_ - min(labels_)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels_, test_size=0.25, random_state=0, stratify=labels_\n",
    ")\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "pipeline_clf = Pipeline(\n",
    "    [\n",
    "        (\"variancethreshold\", VarianceFeatureReduction(threshold=0.05)),\n",
    "        (\"correlationreduction\", CorrelationFeatureReduction()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"classifier\", clf),\n",
    "    ]\n",
    ")\n",
    "pipeline_clf.fit(X_train, y_train)\n",
    "# Predict the values for the test set\n",
    "y_pred = pipeline_clf.predict(X_test)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Convert the report to a DataFrame\n",
    "df = pd.DataFrame(report).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = permutation_importance(pipeline_clf, X_test, y_test, n_repeats=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "        print(\n",
    "            f\"{list(X_test.columns)[i]:<8} \"\n",
    "            f\"{r.importances_mean[i]:.3f}\"\n",
    "            f\" +/- {r.importances_std[i]:.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_ = df.iloc[:4].copy()\n",
    "df1_[\"support\"] = 0.1\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=df1_.values.tolist(),\n",
    "        x=df1.columns,\n",
    "        y=[\"1 and 2\", \"3\", \"4\", \"5\"],\n",
    "        colorscale=[\"#ffffff\", colors[4]],\n",
    "        zmin=0,\n",
    "        zmax=1,\n",
    "        showscale=True,\n",
    "        xgap=1,\n",
    "        ygap=1,\n",
    "        text=[[float_int(val) for val in row] for row in df.iloc[:4].values.tolist()],\n",
    "        texttemplate=\"%{text}\",\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    margin=dict(pad=10),  # padding\n",
    "    plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "    legend_title_text=\"Intervertebral Disc\",\n",
    "    grid_rows=1,\n",
    "    title=\"Per grade classification results for the four level grading task\",\n",
    ")\n",
    "fig.update_xaxes(\n",
    "    automargin=True,\n",
    "    tickvals=list(range(len(df1.columns))),\n",
    "    ticktext=[col.capitalize() for col in df1_.columns],\n",
    "    showgrid=False,\n",
    ")\n",
    "fig.update_yaxes(automargin=True, title_text=\"Pfirrmann Grade\", showgrid=False)\n",
    "fig.update_traces(\n",
    "    textfont_size=16,\n",
    ")\n",
    "fig.show()\n",
    "# fig.write_image(root_dir.joinpath(\"figures\", \"per_grade_4_levels.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_series = pd.Series(y_pred, name=\"Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([pred_series, y_test]).add(2).astype(int)\n",
    "df = df.T.groupby(\"Predicted\").value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    df, title=\"Class Prediction Error\", color_discrete_sequence=colors\n",
    ")  # replace 0 with your column name if needed\n",
    "\n",
    "for name, trace in zip([\"1 and 2\", \"3\", \"4\", \"5\"], fig.data):\n",
    "    trace.name = name\n",
    "\n",
    "total_count = df.sum(axis=1)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df.index,\n",
    "        y=total_count,\n",
    "        mode=\"text\",\n",
    "        text=total_count,\n",
    "        textposition=\"top center\",\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "fig.update_traces(textfont_size=12)\n",
    "fig.update_xaxes(\n",
    "    title_text=\"Predicted Pfirrmann Grade\",\n",
    "    tickvals=list(range(2, 6)),\n",
    "    ticktext=[\"1 and 2\", \"3\", \"4\", \"5\"],\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Frequency\", showgrid=True, gridcolor=\"rgba(184, 184, 184, 0.3)\"\n",
    ")\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "    legend_title_text=\"True Pfirrmann Grade\",\n",
    "    grid_rows=1,\n",
    ")\n",
    "fig.show()\n",
    "# fig.write_image(root_dir.joinpath(\"figures\", \"class_prediction_error_4level.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radiomics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
